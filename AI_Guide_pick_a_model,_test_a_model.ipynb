{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1BQjq56B+XVhFpCoga6GD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyfallsin/MIECO/blob/main/AI_Guide_pick_a_model%2C_test_a_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Steps with Language Models\n",
        "\n",
        "Unlike other guides, this one is designed to:\n",
        "- teach you how to always remain on the bleeding edge of published AI research\n",
        "- not be tied to a closed-source / closed-data large language model (ex OpenAI, Anthropic)\n",
        "- broaden your perspective on what's already out there for any given task\n",
        "- create a data-led system for always identifying and using the state-of-the-art (SOTA) model for any particular task.\n",
        "\n",
        "We're going to hone in on \"text summarization\" as our first task."
      ],
      "metadata": {
        "id": "OPw-GjAas0m0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So... why are we not using an existing LLM?\n",
        "\n",
        "Great question. Most available LLMs worth their salt can do many tasks, including summarization.\n",
        "\n",
        "However, many of them are not open, are trained on undisclosed data and exhibit biases. Responsible AI use require careful choices, and we're here to help you make them.\n",
        "\n",
        "Finally, most large LLMs require powerful GPU compute to use. While there are many models that you can use as a service, most of them cost money per API call. Unnecessary when some of the more common tasks can be done at good quality with already available open models and off-the-shelf hardware."
      ],
      "metadata": {
        "id": "gAihu4Qw-Gzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do using open models matter?\n",
        "\n",
        "Over the last few decades, engineers have been blessed with being able to onboard by starting with open source projects, and eventually shipping open source to production. This default state is now at risk.\n",
        "\n",
        "Yes, there are many open models available that do a great job. However, most guides don't discuss how to get started with them using simple steps and instead bias towards existing closed APIs.\n",
        "\n",
        "Funding is flowing to commercial AI projects, who have larger budgets than open source contributors to market their work, which inevitably leads to engineers starting with closed source projects and shipping expensive closed projects to production."
      ],
      "metadata": {
        "id": "7e-c-RKd_pyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our First Project - Summarization\n",
        "\n",
        "We're going to:\n",
        "- Get some long documents to summarize.\n",
        "- Figure out how to summarize them using the current state-of-the-art open source models.\n",
        "- Write some code to do so."
      ],
      "metadata": {
        "id": "N6LEcWmCt6v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Where can I grab some documents?\n",
        "For simplicity's sake, let's grab a few HTML pages.\n",
        "\n",
        "Note that in the real world, you will likely have use other libraries to extract content for any particular file type."
      ],
      "metadata": {
        "id": "kcb9K3dLubjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, we will import the `requests` library to grab webpages\n",
        "import requests\n",
        "\n",
        "# TODO: replace these URLs\n",
        "urls = ['https://www.cnn.com/2023/08/29/tech/ai-chatbot-hallucinations/index.html']\n",
        "html_pages = [requests.get(url).text for url in urls]"
      ],
      "metadata": {
        "id": "uHgg18k1t4sy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's use the Python HTML parser BeautifulSoup to grab the body text of these pages"
      ],
      "metadata": {
        "id": "Buf0eQHo0msv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "page_content = []\n",
        "\n",
        "for html_page in html_pages:\n",
        "    soup = BeautifulSoup(html_page, 'html.parser')\n",
        "    if soup.body:\n",
        "        for tag in soup.body(['footer', 'div.footer']):\n",
        "          tag.decompose()\n",
        "        page_content.append(soup.body.get_text())\n",
        "\n",
        "#print(page_content[0])"
      ],
      "metadata": {
        "id": "GEHyIa4h0tzL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great. Now we're ready to start summarizing."
      ],
      "metadata": {
        "id": "W5Y9LBzfujnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A brief pause for context.\n",
        "\n",
        "The AI space is moving so fast that it requires a tremendous amount of catching up on scientific papers every week to understand the lay of the land and the state of the art.\n",
        "\n",
        "It's quite difficult for an engineer who is brand new to AI to:\n",
        "* discover which open models are even out there\n",
        "* which models are appropriate for a particular task\n",
        "* which benchmarks are used to evaluate those models\n",
        "* which models are performing well based on evaluations\n",
        "* which models can actually run on available hardware\n",
        "\n",
        "For the working engineer on a deadline, this is problematic. There's not much centralized discourse on working with open source AI models. Instead there are fragmented X (formerly Twitter) threads, random private groups and lots of word-of-mouth transfer.\n",
        "\n",
        "However, once you master a framework on how to address all of the above, you will have the means to forever be on the bleeding age of published AI research.\n"
      ],
      "metadata": {
        "id": "DM2n6byeDHss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do I get a list of available open summarization models?\n",
        "\n",
        "For now, we recommend [Huggingface](https://huggingface.co/models?pipeline_tag=summarization) and their large directory of open models broken down by task. This is a great starting point. Note that larger LLMs are also included in these lists, so we will have to filter.\n",
        "\n",
        "In this huge list of summarization models, which ones do we choose?\n",
        "\n",
        "We don't know what any of these models are trained on. For example, a summarizer trained on news articles vs Reddit posts will perform better on news articles.\n",
        "\n",
        "What we need is a set of metrics and benchmarks that we can use to do apples-to-apples comparisons of these models."
      ],
      "metadata": {
        "id": "oG-p4zhyJ35-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do I evaluate summarization models?\n",
        "\n",
        "These steps below can be used to evaluate any available model for any task. It requires hopping between a few sources of data for now, but we will be making this a lot easier moving forward.\n",
        "\n",
        "Steps:\n",
        "1. Find the most common datasets used to train models for summarization.\n",
        "2. Find the most common metrics used to evaluate models for summarization across those datasets."
      ],
      "metadata": {
        "id": "LJ0kYfaHYypn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding datasets\n",
        "\n",
        "The easiest way to do this is using _[Papers With Code](https://paperswithcode.com/methods)_, an excellent resource for finding the latest scientific papers by task that also have code repositories attached.\n",
        "\n",
        "First, filter _Papers With Code_ datasets [by most cited text-based English datasets](https://paperswithcode.com/datasets?q=&v=lst&o=cited&lang=english&mod=texts&task=text-summarization&page=1)\n",
        "\n",
        "Let's pick (as of this writing) the most cited dataset -- the \"[CNN/DailyMail](https://paperswithcode.com/dataset/cnn-daily-mail-1)\" dataset. Usually most cited is one marker of popularity.\n",
        "\n",
        "Now, you don't need to download this dataset. But we're going to review the info _Papers With Code_ have provided to learn more about it for the next step. This dataset is also available on [Huggingface](https://huggingface.co/datasets/cnn_dailymail).\n",
        "\n",
        "First, check the license. In this case, it's MIT licensed, which means it can be used for both commercial and personal projects.\n",
        "\n",
        "Next, see if the papers using this dataset are recent. You can do this by sorting Papers in descending order. This particular dataset has many papers from 2023 - great!\n",
        "\n",
        "Now, let's dig into how we can evaluate models that use this dataset.\n"
      ],
      "metadata": {
        "id": "Uv6_QnqCmHu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating models\n",
        "\n",
        "Next, we look for measured metrics that are common across datasets for the summarization task. BUT, if you're not familiar with the literature on summarization, you have no idea what those are.\n",
        "\n",
        "To find out, pick a \"Subtask\" that's close to what you'd like to see. We'd like to summarize the CNN article we pulled down above, so let's choose \"[Document Summarization](https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail)\".\n",
        "\n",
        "Now we're in business! This page contains a significant amount of new information.\n",
        "\n",
        "There are mentions of three new terms: ROUGE-1, ROUGE-2 and ROUGE-L. These are the metrics that are used to [measure summarization performance](https://en.wikipedia.org/wiki/ROUGE_(metric)).\n",
        "\n",
        "There are also a list of models and their scores on these three metrics - this is exactly what we're looking for.\n",
        "\n",
        "Assuming we're looking at ROUGE-1 as our metric, we now have the top 3 models that we can evaluate in more detail. All 3 are close to 50, which is a promising ROUGE score (read up on ROUGE)"
      ],
      "metadata": {
        "id": "fCXBmqwZmMAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing out a model\n",
        "\n",
        "OK, we have a few candidates, so let's pick a model that will run on our local machines. Many models get their best performance when running on GPUs, but there are many that also generate summaries fast on CPUs. Let's pick one of those to start - Google's Pegasus."
      ],
      "metadata": {
        "id": "1D3Kq1wlqPWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we install huggingface's transformers library\n",
        "%pip install transformers sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Ay3KNetit6",
        "outputId": "ff07bfd7-6bf8-4524-a5ca-e8e471921191"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.33.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we [find Pegasus](https://huggingface.co/google/pegasus-cnn_dailymail) on Huggingface. Cool, there's a version trained entirely on the CNN/DailyMail dataset."
      ],
      "metadata": {
        "id": "0Bv1cXlft3Tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "\n",
        "#src_text = [\n",
        "#    \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\n",
        "#]\n",
        "\n",
        "content_to_summarize = page_content[0]\n",
        "\n",
        "# first we choose GPUs if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# load the model from Huggingface\n",
        "model_name = \"google/pegasus-cnn_dailymail\"\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# setup the tokenizer to tokenize the text\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "batch = tokenizer(content_to_summarize, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "# now call the model to summarize the text\n",
        "summarized = model.generate(**batch)\n",
        "\n",
        "# run it through the decoder\n",
        "summarized_text = tokenizer.batch_decode(summarized, skip_special_tokens=True)\n",
        "\n",
        "print()\n",
        "print(summarized_text[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Wg603Wtmln",
        "outputId": "36b674aa-b1c8-411e-dbb3-8a911df283a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ChatGPT has mesmerized us with its ability to produce authoritative, human-sounding responses to seemingly any prompt.<n>But as more people turn to this buzzy technology for things like homework help, workplace research, or health inquiries, one of its biggest pitfalls is becoming increasingly apparent.<n>Researchers have come to refer to this tendency of AI models to spew inaccurate information as hallucinations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZEia9Fnx86x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}